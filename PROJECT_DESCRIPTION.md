# Project Description

## Short Description (for package.json)
Modern TypeScript SDK for integrating with the LessTokens token compression API. Compress prompts before sending to LLM providers (OpenAI, Anthropic, Google, DeepSeek) to reduce token usage and costs while maintaining response quality.

## Medium Description (for GitHub/NPM)
A modern, type-safe TypeScript SDK that integrates with the LessTokens token compression API to optimize LLM interactions. Compress prompts before sending them to major LLM providers (OpenAI, Anthropic, Google, DeepSeek) to significantly reduce token usage and costs. Features include streaming support, multi-turn conversations, full provider API compatibility, and comprehensive error handling.

## Long Description (for documentation)
The LessTokens SDK is a production-ready TypeScript library designed to optimize your LLM API usage by compressing prompts before sending them to language model providers. By integrating with the LessTokens compression API, the SDK can reduce token consumption by up to 50% or more while maintaining the semantic meaning and quality of your prompts.

**Key Features:**
- **Token Compression**: Automatically compress prompts via LessTokens API before sending to LLMs
- **Multi-Provider Support**: Seamlessly works with OpenAI, Anthropic, Google Gemini, and DeepSeek
- **Full API Compatibility**: All provider-specific options and features are supported through official SDKs
- **Streaming Support**: Real-time streaming responses with compression metrics
- **Multi-turn Conversations**: Support for conversation history and context management
- **Customizable Messages**: Customize message roles and content with compression statistics
- **Type-Safe**: Fully typed with TypeScript for better developer experience
- **Comprehensive Metrics**: Detailed token usage and savings tracking
- **Production Ready**: Extensive test coverage (96%+), error handling, and documentation

**Use Cases:**
- Reduce LLM API costs by compressing long prompts
- Optimize token usage in production applications
- Maintain conversation quality while reducing expenses
- Integrate compression seamlessly into existing LLM workflows

Perfect for developers building AI applications who want to optimize costs without sacrificing functionality or response quality.



